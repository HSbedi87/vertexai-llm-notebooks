{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Objective\n",
        "\n",
        "This notebook demonstrates how you use text embedding provided by Vertex AI PaLM to index code files and retrieve using natural language queries."
      ],
      "metadata": {
        "id": "Z9zJw-Fkckyb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## High Level Flow\n",
        "\n",
        "Index Creation:\n",
        "- Recursively list the files(.ipynb & .py) in github repo\n",
        "- Extract code and markdown from the files\n",
        "- Generate embeddings for each code strings\n",
        "- Add embedding to the vector store\n",
        "\n",
        "Model Prompting:\n",
        "- User enters a prompt or asks a question as a prompt\n",
        "- Generated embedding for the user prompt to capture semantics\n",
        "- Search the vector store (SCANN) to retrieve the nearest embeddings (relevant documents) closer to the prompt\n",
        "- Provide the urls for the matches files\n",
        "\n"
      ],
      "metadata": {
        "id": "X52KdopXcgKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-cloud-aiplatform --upgrade --user\n",
        "\n",
        "!pip install scann"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9FK_-nyy-Xd",
        "outputId": "9306765d-a5dd-4a63-ebe2-f9351c7fcf63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: google-cloud-aiplatform in /root/.local/lib/python3.10/site-packages (1.25.0)\n",
            "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.11.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (1.22.2)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (3.19.6)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (23.1)\n",
            "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.8.0)\n",
            "Requirement already satisfied: google-cloud-bigquery<4.0.0dev,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (3.9.0)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /root/.local/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.10.1)\n",
            "Requirement already satisfied: shapely<2.0.0 in /root/.local/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.8.5.post1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.59.0)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.17.3)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.27.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.54.0)\n",
            "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.48.2)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.3.2)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.5.0)\n",
            "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.8.2)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /root/.local/lib/python3.10/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.12.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (4.9)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (3.4)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (0.5.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scann in /usr/local/lib/python3.10/dist-packages (1.2.9)\n",
            "Requirement already satisfied: tensorflow~=2.11.0 in /usr/local/lib/python3.10/dist-packages (from scann) (2.11.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from scann) (1.22.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from scann) (1.16.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.11.0->scann) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.11.0->scann) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.11.0->scann) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.11.0->scann) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.11.0->scann) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.11.0->scann) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.11.0->scann) (3.8.0)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.11.0->scann) (2.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.11.0->scann) (16.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.11.0->scann) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.11.0->scann) (23.1)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.11.0->scann) (3.19.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.11.0->scann) (67.7.2)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.11.0->scann) (2.11.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.11.0->scann) (2.11.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.11.0->scann) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.11.0->scann) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.11.0->scann) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.11.0->scann) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow~=2.11.0->scann) (0.40.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->scann) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->scann) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->scann) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->scann) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->scann) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->scann) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->scann) (2.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->scann) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->scann) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->scann) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->scann) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->scann) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->scann) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->scann) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->scann) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->scann) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->scann) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->scann) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GITHUB_TOKEN = \"\"\n",
        "GITHUB_REPO = \"GoogleCloudPlatform/vertex-ai-samples\"\n",
        "PROJECT_ID = \"hsbedi-docai\"\n",
        "LOCATION = \"us-central1\""
      ],
      "metadata": {
        "id": "LPj8TUErytzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests, time\n",
        "\n",
        "#crawls a GitHub repository and returns a list of all files in the repository\n",
        "def crawl_github_repo(url,is_sub_dir,access_token = f\"{GITHUB_TOKEN}\"):\n",
        "\n",
        "    ignore_list = ['__init__.py']\n",
        "\n",
        "    if not is_sub_dir:\n",
        "\n",
        "        api_url = f\"https://api.github.com/repos/{url}/contents\"\n",
        "\n",
        "    else:\n",
        "\n",
        "        api_url = url\n",
        "\n",
        "    headers = {\n",
        "        \"Accept\": \"application/vnd.github.v3+json\",\n",
        "        \"Authorization\": f\"Bearer {access_token}\"\n",
        "                   }\n",
        "\n",
        "    response = requests.get(api_url, headers=headers)\n",
        "    response.raise_for_status()  # Check for any request errors\n",
        "\n",
        "    files = []\n",
        "\n",
        "    contents = response.json()\n",
        "    # print(f\"{contents}\")\n",
        "\n",
        "    for item in contents:\n",
        "        # if item['type'] == 'file' and item['name'] not in ignore_list and (item['name'].endswith('.py') or item['name'].endswith('.ipynb')):\n",
        "        if item['type'] == 'file' and item['name'] not in ignore_list and (item['name'].endswith('.ipynb')):\n",
        "            files.append(item['html_url'])\n",
        "        elif item['type'] == 'dir' and not item['name'].startswith(\".\"):\n",
        "            sub_files = crawl_github_repo(item['url'],True)\n",
        "            time.sleep(.1)\n",
        "            files.extend(sub_files)\n",
        "\n",
        "    return files"
      ],
      "metadata": {
        "id": "1zYHHgLCxyHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "code_files_urls = crawl_github_repo(GITHUB_REPO,False,GITHUB_TOKEN)\n",
        "\n",
        "# Write list to a file\n",
        "with open('file.txt', 'w') as f:\n",
        "    for item in code_files_urls:\n",
        "        f.write(item + '\\n')\n",
        "\n",
        "\n",
        "len(code_files_urls)"
      ],
      "metadata": {
        "id": "ScasIvMMxcmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Authenticate with Google Cloud credentials for Google colab\n",
        "from google.colab import auth as google_auth\n",
        "google_auth.authenticate_user()"
      ],
      "metadata": {
        "id": "NtrQxgVDz1KL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize the Vertex AI LL Models\n",
        "from vertexai.preview.language_models import TextEmbeddingModel\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "aiplatform.init(project=PROJECT_ID, location=LOCATION)\n",
        "\n",
        "embedding_model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@001\")"
      ],
      "metadata": {
        "id": "qY0GXjMhyUqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('file.txt') as f:\n",
        "    files = f.read().splitlines()\n",
        "len(files)"
      ],
      "metadata": {
        "id": "v3jAXO6i2ZIp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2cde7e6-73d1-4dd2-ecd1-5c66e8bfede1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "449"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import nbformat\n",
        "import json\n",
        "\n",
        "# Extracts the python code from an ipynb file from github\n",
        "def extract_python_code_from_ipynb(github_url):\n",
        "    raw_url = github_url.replace(\"github.com\", \"raw.githubusercontent.com\").replace(\"/blob/\", \"/\")\n",
        "\n",
        "    response = requests.get(raw_url)\n",
        "    response.raise_for_status()  # Check for any request errors\n",
        "\n",
        "    notebook_content = response.text\n",
        "\n",
        "    notebook = nbformat.reads(notebook_content, as_version=nbformat.NO_CONVERT)\n",
        "\n",
        "    python_code = None\n",
        "\n",
        "    for cell in notebook.cells:\n",
        "        if cell.cell_type == \"code\":\n",
        "          if not python_code:\n",
        "            python_code = cell.source\n",
        "          else:\n",
        "            python_code += \"\\n\" + cell.source\n",
        "\n",
        "    return python_code\n",
        "\n",
        "def extract_python_code_from_py(github_url):\n",
        "    raw_url = github_url.replace(\"github.com\", \"raw.githubusercontent.com\").replace(\"/blob/\", \"/\")\n",
        "\n",
        "    response = requests.get(raw_url)\n",
        "    response.raise_for_status()  # Check for any request errors\n",
        "\n",
        "    python_code = response.text\n",
        "\n",
        "    return python_code"
      ],
      "metadata": {
        "id": "jd_yEnZM3ce0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "code_strings = []\n",
        "\n",
        "for i in range(0, len (code_files_urls)):\n",
        "    if code_files_urls[i].endswith(\".ipynb\"):\n",
        "        code_strings.append(extract_python_code_from_ipynb(code_files_urls[i]))\n",
        "    else:\n",
        "        code_strings.append((extract_python_code_from_py(code_files_urls[i])))"
      ],
      "metadata": {
        "id": "pQH3-5ze7WD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "code_embeddings = []\n",
        "BATCH_SIZE = 5\n",
        "\n",
        "for batch_start in range(0, len(code_strings), BATCH_SIZE):\n",
        "    batch_end = batch_start + BATCH_SIZE\n",
        "    batch = code_strings[batch_start:batch_end]\n",
        "    print(f\"Batch {batch_start} to {batch_end-1}\")\n",
        "\n",
        "    batch_embeddings = [emb.values for emb in embedding_model.get_embeddings(batch)]  # get embeddings for each batch\n",
        "\n",
        "    # batch_embeddings = [e[\"embedding\"] for e in embedding_model.get_embeddings(df.text.values)]\n",
        "    code_embeddings.extend(batch_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPgebXLM7YQH",
        "outputId": "114bc9d1-7211-45bf-bce9-b03183a3c599"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0 to 4\n",
            "Batch 5 to 9\n",
            "Batch 10 to 14\n",
            "Batch 15 to 19\n",
            "Batch 20 to 24\n",
            "Batch 25 to 29\n",
            "Batch 30 to 34\n",
            "Batch 35 to 39\n",
            "Batch 40 to 44\n",
            "Batch 45 to 49\n",
            "Batch 50 to 54\n",
            "Batch 55 to 59\n",
            "Batch 60 to 64\n",
            "Batch 65 to 69\n",
            "Batch 70 to 74\n",
            "Batch 75 to 79\n",
            "Batch 80 to 84\n",
            "Batch 85 to 89\n",
            "Batch 90 to 94\n",
            "Batch 95 to 99\n",
            "Batch 100 to 104\n",
            "Batch 105 to 109\n",
            "Batch 110 to 114\n",
            "Batch 115 to 119\n",
            "Batch 120 to 124\n",
            "Batch 125 to 129\n",
            "Batch 130 to 134\n",
            "Batch 135 to 139\n",
            "Batch 140 to 144\n",
            "Batch 145 to 149\n",
            "Batch 150 to 154\n",
            "Batch 155 to 159\n",
            "Batch 160 to 164\n",
            "Batch 165 to 169\n",
            "Batch 170 to 174\n",
            "Batch 175 to 179\n",
            "Batch 180 to 184\n",
            "Batch 185 to 189\n",
            "Batch 190 to 194\n",
            "Batch 195 to 199\n",
            "Batch 200 to 204\n",
            "Batch 205 to 209\n",
            "Batch 210 to 214\n",
            "Batch 215 to 219\n",
            "Batch 220 to 224\n",
            "Batch 225 to 229\n",
            "Batch 230 to 234\n",
            "Batch 235 to 239\n",
            "Batch 240 to 244\n",
            "Batch 245 to 249\n",
            "Batch 250 to 254\n",
            "Batch 255 to 259\n",
            "Batch 260 to 264\n",
            "Batch 265 to 269\n",
            "Batch 270 to 274\n",
            "Batch 275 to 279\n",
            "Batch 280 to 284\n",
            "Batch 285 to 289\n",
            "Batch 290 to 294\n",
            "Batch 295 to 299\n",
            "Batch 300 to 304\n",
            "Batch 305 to 309\n",
            "Batch 310 to 314\n",
            "Batch 315 to 319\n",
            "Batch 320 to 324\n",
            "Batch 325 to 329\n",
            "Batch 330 to 334\n",
            "Batch 335 to 339\n",
            "Batch 340 to 344\n",
            "Batch 345 to 349\n",
            "Batch 350 to 354\n",
            "Batch 355 to 359\n",
            "Batch 360 to 364\n",
            "Batch 365 to 369\n",
            "Batch 370 to 374\n",
            "Batch 375 to 379\n",
            "Batch 380 to 384\n",
            "Batch 385 to 389\n",
            "Batch 390 to 394\n",
            "Batch 395 to 399\n",
            "Batch 400 to 404\n",
            "Batch 405 to 409\n",
            "Batch 410 to 414\n",
            "Batch 415 to 419\n",
            "Batch 420 to 424\n",
            "Batch 425 to 429\n",
            "Batch 430 to 434\n",
            "Batch 435 to 439\n",
            "Batch 440 to 444\n",
            "Batch 445 to 449\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(code_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K860H6eCDrJw",
        "outputId": "88e1bd41-4bbb-401e-a344-3c9907029f71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "449"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SCANN Index Creation"
      ],
      "metadata": {
        "id": "V1yDydOtP0fo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Change the num_leaves and training_sample size based on your corpus\n",
        "import timeit\n",
        "import scann\n",
        "import numpy as np\n",
        "start = timeit.default_timer()\n",
        "normalized_dataset = code_embeddings / np.linalg.norm(code_embeddings, axis=1)[:, np.newaxis]\n",
        "searcher = scann.scann_ops_pybind.builder(normalized_dataset, 10, \"dot_product\").tree(\n",
        "    num_leaves=200, num_leaves_to_search=100, training_sample_size=1000).score_ah(\n",
        "    2, anisotropic_quantization_threshold=0.2).reorder(100).build()\n",
        "elapsed = timeit.default_timer() -start\n",
        "#Printing the Elapsed Time\n",
        "elapsed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFmKloStP4mh",
        "outputId": "dd48fd70-d577-4022-c871-81324c43e83e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.44209675900037837"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to query the SCANN Index. We use 3 Neighbors to return\n",
        "#Increasing the neighbors will slow the query down\n",
        "import timeit\n",
        "def search_posts(query_embedding, num_results=5):\n",
        "    start = timeit.default_timer()\n",
        "\n",
        "    neighbors, distances = searcher.search(query_embedding, final_num_neighbors=num_results) #change the number of neighbors for number of docs returned.\n",
        "    elapsed = timeit.default_timer() -start\n",
        "    return neighbors, distances, elapsed\n"
      ],
      "metadata": {
        "id": "6usrWIH4QB8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Just testing the SCANN API\n",
        "queries = [\"model training profiling for tensorflow\",\n",
        "           \"Recomender system\",\n",
        "           \"pytorch sample for Text classification\"]\n",
        "query_embeddings = embedding_model.get_embeddings(queries)"
      ],
      "metadata": {
        "id": "5Qb1cPX2Q3cp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, len(queries)):\n",
        "    print(f\"Query: {queries[i]}\")\n",
        "    neighbors, distances, elapsed  = search_posts(query_embeddings[i].values,5)\n",
        "    print(neighbors)\n",
        "    print(distances)\n",
        "\n",
        "    for result_index in neighbors:\n",
        "        print(files[result_index])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_v9XiauRUoH",
        "outputId": "190a9a27-8008-4dad-b58b-b1475fbe829f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: model training profiling for tensorflow\n",
            "[251 242 324 422   9]\n",
            "[0.72467923 0.72459215 0.7197988  0.7195308  0.715413  ]\n",
            "https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/sdk/SDK_Explainable_AI_Custom_Tabular.ipynb\n",
            "https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/reduction_server/distributed-training-reduction-server.ipynb\n",
            "https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/custom/custom_training_tensorboard_profiler.ipynb\n",
            "https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/tensorboard/tensorboard_profiler_custom_training.ipynb\n",
            "https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/community-content/tf_agents_bandits_movie_recommendation_with_kfp_and_vertex_sdk/step_by_step_sdk_tf_agents_bandits_movie_recommendation/step_by_step_sdk_tf_agents_bandits_movie_recommendation.ipynb\n",
            "Query: Recomender system\n",
            "[  9 448 434 385 388]\n",
            "[0.63269866 0.6294222  0.6284186  0.62738097 0.62724984]\n",
            "https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/community-content/tf_agents_bandits_movie_recommendation_with_kfp_and_vertex_sdk/step_by_step_sdk_tf_agents_bandits_movie_recommendation/step_by_step_sdk_tf_agents_bandits_movie_recommendation.ipynb\n",
            "https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/workbench/subscriber_churn_prediction/telecom-subscriber-churn-prediction.ipynb\n",
            "https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/vizier/gapic-vizier-multi-objective-optimization.ipynb\n",
            "https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/model_monitoring/model_monitoring.ipynb\n",
            "https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/pipelines/automl_tabular_classification_beans.ipynb\n",
            "Query: pytorch sample for Text classification\n",
            "[  6   4 410 431 240]\n",
            "[0.75869316 0.7371373  0.73495895 0.7289058  0.727823  ]\n",
            "https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/community-content/pytorch_text_classification_using_vertex_sdk_and_gcloud/pytorch-text-classification-vertex-ai-train-tune-deploy.ipynb\n",
            "https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/community-content/pytorch_pre_built_images_deployment/pytorch-text-classification-vertex-ai-deploy.ipynb\n",
            "https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/reduction_server/pytorch_distributed_training_reduction_server.ipynb\n",
            "https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/training/pytorch-text-sentiment-classification-custom-train-deploy.ipynb\n",
            "https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/prediction/custom_prediction_routines/SDK_Triton_PyTorch_Local_Prediction.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fdzZmUFEkniw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}