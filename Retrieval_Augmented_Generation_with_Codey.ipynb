{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2023 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "metadata": {
        "id": "5norOZI0mA6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use Retrieval Augmented Generation (RAG) with Codey API's\n",
        "\n",
        "<table align=\"left\">\n",
        "\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/language/use-cases/code-generation/retrieval_augmented_generation_with_codey\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/use-cases/code-generation/retrieval_augmented_generation_with_codey\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/blob/main/language/use-cases/code-generation/retrieval_augmented_generation_with_codey\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "      Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ],
      "metadata": {
        "id": "XNPE46X8mJj4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Objective\n",
        "\n",
        "This notebook demonstrates how you augment output from Codey APIs by bringing in external knowledge. We'll show you an example using Code Retrieval Augmented Generation(RAG) pattern using [Google Cloud's Generative AI github repository](https://github.com/GoogleCloudPlatform/generative-ai) as external knowledge.The notebook uses [Vertex AI PaLM API for Code](https://cloud.google.com/vertex-ai/docs/generative-ai/code/code-models-overview) , [Embeddings for Text API](https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-text-embeddings), FAISS vector store and [LangChain ü¶úÔ∏èüîó](https://python.langchain.com/en/latest/).\n",
        "\n",
        "### Overview\n",
        "\n",
        "Here is overview of what we'll go over.\n",
        "\n",
        "Index Creation:\n",
        "\n",
        "1. Recursively list the files(.ipynb) in github repo\n",
        "2. Extract code and markdown from the files\n",
        "3. Chunk & generate embeddings for each code strings and add initialize the vector store\n",
        "\n",
        "Runtime:\n",
        "\n",
        "4. User enters a prompt or asks a question as a prompt\n",
        "5. Try zero-shot prompt\n",
        "6. Run prompt using RAG Chain & compare results.To generate response we use **code-bison** however can also use **code-gecko** and **codechat-bison**\n",
        "\n",
        "### Cost\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "- Vertex AI PaLM APIs offered by Google Cloud\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage.\n",
        "\n",
        "**Note:** We are using local vector store(FAISS) for this example however recommend managed highly scalable vector store for production usage such as [Vertex AI Matching Engine](https://cloud.google.com/vertex-ai/docs/vector-search/overview) or [AlloyDB for PostgreSQ](https://cloud.google.com/alloydb/docs/ai/work-with-embeddings) or [Cloud SQL for PostgreSQL](https://cloud.google.com/sql/docs/postgres/features)  using pgvector extension."
      ],
      "metadata": {
        "id": "zNAEdYNFmQcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install libraries"
      ],
      "metadata": {
        "id": "-XXl6qTJMqxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --user langchain google-cloud-aiplatform faiss-cpu"
      ],
      "metadata": {
        "id": "QHaqV20Csqkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Restart runtime"
      ],
      "metadata": {
        "id": "-VUWOgz6M1rZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Restart kernel after installs so that your environment can access the new packages\n",
        "import IPython\n",
        "import time\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ],
      "metadata": {
        "id": "BIS8EYgkMy8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Authenticate your notebook environment (Colab only)"
      ],
      "metadata": {
        "id": "iEDmABVkNBr2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are running this notebook on Google Colab, you will need to authenticate your environment. To do this, run the new cell below. This step is not required if you are using Vertex AI Workbench."
      ],
      "metadata": {
        "id": "uZcP9WBENG0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    # Define project information\n",
        "    PROJECT_ID = \"\"  # @param {type:\"string\"}\n",
        "    LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "\n",
        "    # Authenticate user to Google Cloud\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ],
      "metadata": {
        "id": "1S_HgQXQNcbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import libraries"
      ],
      "metadata": {
        "id": "rVmxMr43Nhoo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-Tljm5asMBc",
        "outputId": "3e69672a-f494-42be-876e-c3378d1b80a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vertex AI SDK version: 1.36.1\n"
          ]
        }
      ],
      "source": [
        "# Utils\n",
        "\n",
        "# LangChain\n",
        "from langchain.llms import VertexAI\n",
        "from langchain.embeddings import VertexAIEmbeddings\n",
        "\n",
        "from langchain.schema import HumanMessage, SystemMessage\n",
        "from langchain.schema.document import Document\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.text_splitter import Language\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "import time\n",
        "from typing import List\n",
        "\n",
        "from pydantic import BaseModel\n",
        "\n",
        "# Vertex AI\n",
        "from google.cloud import aiplatform\n",
        "import vertexai\n",
        "from vertexai.language_models import CodeGenerationModel\n",
        "\n",
        "print(f\"Vertex AI SDK version: {aiplatform.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize project\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "\n",
        "GITHUB_TOKEN = \"\" # @param {type:\"string\"}\n",
        "GITHUB_REPO = \"GoogleCloudPlatform/generative-ai\" # @param {type:\"string\"}\n",
        "\n",
        "\n",
        "#Code Generation\n",
        "\n",
        "code_llm = VertexAI(\n",
        "    model_name=\"code-bison@latest\",\n",
        "    max_output_tokens=2048,\n",
        "    temperature=0.1,\n",
        "    verbose=False,\n",
        "    )\n"
      ],
      "metadata": {
        "id": "eNGEcBKG0iK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Index Creation\n",
        "### 1. Recursively list the files(.ipynb) in the github rep"
      ],
      "metadata": {
        "id": "dqq3GeEbOJbU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests, time\n",
        "\n",
        "#Crawls a GitHub repository and returns a list of all ipynb files in the repository\n",
        "def crawl_github_repo(url,is_sub_dir,access_token = f\"{GITHUB_TOKEN}\"):\n",
        "\n",
        "    ignore_list = ['__init__.py']\n",
        "\n",
        "    if not is_sub_dir:\n",
        "\n",
        "        api_url = f\"https://api.github.com/repos/{url}/contents\"\n",
        "\n",
        "    else:\n",
        "\n",
        "        api_url = url\n",
        "\n",
        "    headers = {\n",
        "        \"Accept\": \"application/vnd.github.v3+json\",\n",
        "        \"Authorization\": f\"Bearer {access_token}\"\n",
        "                   }\n",
        "\n",
        "    response = requests.get(api_url, headers=headers)\n",
        "    response.raise_for_status()  # Check for any request errors\n",
        "\n",
        "    files = []\n",
        "\n",
        "    contents = response.json()\n",
        "    # print(f\"{contents}\")\n",
        "\n",
        "    for item in contents:\n",
        "        if item['type'] == 'file' and item['name'] not in ignore_list and (item['name'].endswith('.py') or item['name'].endswith('.ipynb')):\n",
        "            files.append(item['html_url'])\n",
        "        elif item['type'] == 'dir' and not item['name'].startswith(\".\"):\n",
        "            sub_files = crawl_github_repo(item['url'],True)\n",
        "            time.sleep(.1)\n",
        "            files.extend(sub_files)\n",
        "\n",
        "    return files"
      ],
      "metadata": {
        "id": "eTA1Jt0uOX8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "code_files_urls = crawl_github_repo(GITHUB_REPO,False,GITHUB_TOKEN)\n",
        "\n",
        "# Write list to a file so you do not have to download each time\n",
        "with open('code_files_urls.txt', 'w') as f:\n",
        "    for item in code_files_urls:\n",
        "        f.write(item + '\\n')\n",
        "\n",
        "\n",
        "len(code_files_urls)"
      ],
      "metadata": {
        "id": "5vaKaxcGO_R6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf2075be-8209-4dce-a922-734a047a7993"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "93"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "code_files_urls[0:10]"
      ],
      "metadata": {
        "id": "c5hoNYJ5byMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility functions for Embeddings API with rate limiting\n",
        "def rate_limit(max_per_minute):\n",
        "    period = 60 / max_per_minute\n",
        "    print(\"Waiting\")\n",
        "    while True:\n",
        "        before = time.time()\n",
        "        yield\n",
        "        after = time.time()\n",
        "        elapsed = after - before\n",
        "        sleep_time = max(0, period - elapsed)\n",
        "        if sleep_time > 0:\n",
        "            print(\".\", end=\"\")\n",
        "            time.sleep(sleep_time)\n",
        "\n",
        "\n",
        "class CustomVertexAIEmbeddings(VertexAIEmbeddings, BaseModel):\n",
        "    requests_per_minute: int\n",
        "    num_instances_per_batch: int\n",
        "\n",
        "    # Overriding embed_documents method\n",
        "    def embed_documents(self, texts: List[str]):\n",
        "        limiter = rate_limit(self.requests_per_minute)\n",
        "        results = []\n",
        "        docs = list(texts)\n",
        "\n",
        "        while docs:\n",
        "            # Working in batches because the API accepts maximum 5\n",
        "            # documents per request to get embeddings\n",
        "            head, docs = (\n",
        "                docs[: self.num_instances_per_batch],\n",
        "                docs[self.num_instances_per_batch :],\n",
        "            )\n",
        "            chunk = self.client.get_embeddings(head)\n",
        "            results.extend(chunk)\n",
        "            next(limiter)\n",
        "\n",
        "        return [r.values for r in results]\n"
      ],
      "metadata": {
        "id": "fizQVzV3sU7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Extract code and from the Jupyter notebooks.\n",
        "You could also include .py file, shell scripts etc."
      ],
      "metadata": {
        "id": "mFNVieLnR8Ie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import nbformat\n",
        "import json\n",
        "\n",
        "# Extracts the python code from an ipynb file from github\n",
        "def extract_python_code_from_ipynb(github_url,cell_type = \"code\"):\n",
        "    raw_url = github_url.replace(\"github.com\", \"raw.githubusercontent.com\").replace(\"/blob/\", \"/\")\n",
        "\n",
        "    response = requests.get(raw_url)\n",
        "    response.raise_for_status()  # Check for any request errors\n",
        "\n",
        "    notebook_content = response.text\n",
        "\n",
        "    notebook = nbformat.reads(notebook_content, as_version=nbformat.NO_CONVERT)\n",
        "\n",
        "    python_code = None\n",
        "\n",
        "    for cell in notebook.cells:\n",
        "        if cell.cell_type == cell_type:\n",
        "          if not python_code:\n",
        "            python_code = cell.source\n",
        "          else:\n",
        "            python_code += \"\\n\" + cell.source\n",
        "\n",
        "    return python_code\n",
        "\n",
        "def extract_python_code_from_py(github_url):\n",
        "    raw_url = github_url.replace(\"github.com\", \"raw.githubusercontent.com\").replace(\"/blob/\", \"/\")\n",
        "\n",
        "    response = requests.get(raw_url)\n",
        "    response.raise_for_status()  # Check for any request errors\n",
        "\n",
        "    python_code = response.text\n",
        "\n",
        "    return python_code"
      ],
      "metadata": {
        "id": "ZsM1M4hn4cBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('code_files_urls.txt') as f:\n",
        "    code_files_urls = f.read().splitlines()\n",
        "len(code_files_urls)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCRp5Xtb48is",
        "outputId": "68397ac8-6779-4923-e94c-3b32ecedcd21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "code_strings = []\n",
        "\n",
        "for i in range(0, len (code_files_urls)):\n",
        "    if code_files_urls[i].endswith(\".ipynb\"):\n",
        "        content = extract_python_code_from_ipynb(code_files_urls[i],\"code\")\n",
        "        doc = Document(page_content=content, metadata= {\"url\": code_files_urls[i], \"file_index\":i})\n",
        "        code_strings.append(doc)"
      ],
      "metadata": {
        "id": "4Y9SMO7H4xgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Chunk & generate embeddings for each code strings & initialize the vector store"
      ],
      "metadata": {
        "id": "T1AF3fhBSLOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chunk code strings\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_language(\n",
        "    language=Language.PYTHON,chunk_size=2000, chunk_overlap=200\n",
        ")\n",
        "\n",
        "\n",
        "texts = text_splitter.split_documents(code_strings)\n",
        "print(len(texts))\n",
        "\n",
        "#Initialize Embedding API\n",
        "EMBEDDING_QPM = 100\n",
        "EMBEDDING_NUM_BATCH = 5\n",
        "embeddings = CustomVertexAIEmbeddings(\n",
        "    requests_per_minute=EMBEDDING_QPM,\n",
        "    num_instances_per_batch=EMBEDDING_NUM_BATCH,\n",
        "    model_name = \"textembedding-gecko@latest\"\n",
        ")\n",
        "\n",
        "# Create Index from embedded code chunks\n",
        "db = FAISS.from_documents(texts, embeddings)\n",
        "\n",
        "# Init your retriever.\n",
        "retriever = db.as_retriever(\n",
        "    search_type=\"similarity\",  # Also test \"similarity\", \"mmr\"\n",
        "    search_kwargs={\"k\": 5},)\n",
        "\n",
        "retriever"
      ],
      "metadata": {
        "id": "oae37l-pvzZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Runtime\n",
        "### 4. User enters a prompt or asks a question as a prompt"
      ],
      "metadata": {
        "id": "Q_gn89IyuHIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_question = \"Create python function that takes a prompt and predicts using langchain.llms interface with VertexAI text-bison model\""
      ],
      "metadata": {
        "id": "1vrvTkO7uFNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define prompt templates\n",
        "\n",
        "\n",
        "# Zero Shot prompt template\n",
        "prompt_zero_shot = \"\"\"\n",
        "    You are a proficient python developer. Respond with the syntactically correct & concise code for to the question below.\n",
        "\n",
        "    Question:\n",
        "    {question}\n",
        "\n",
        "    Output Code :\n",
        "    \"\"\"\n",
        "\n",
        "prompt_prompt_zero_shot = PromptTemplate(\n",
        "input_variables=[\"question\"],\n",
        "template=prompt_zero_shot,\n",
        ")\n",
        "\n",
        "\n",
        "# RAG template\n",
        "prompt_RAG = \"\"\"\n",
        "    You are a proficient python developer. Respond with the syntactically correct code for to the question below. Make sure you follow these rules:\n",
        "    1. Use context to understand the APIs and how to use it & apply.\n",
        "    2. Do not add license information to the output code.\n",
        "    3. Do not include colab code in the output.\n",
        "    4. Ensure all the requirements in the question are met.\n",
        "\n",
        "    Question:\n",
        "    {question}\n",
        "\n",
        "    Context:\n",
        "    {context}\n",
        "\n",
        "    Helpful Response :\n",
        "    \"\"\"\n",
        "\n",
        "prompt_RAG_tempate = PromptTemplate(\n",
        "    template=prompt_RAG, input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "qa_chain = RetrievalQA.from_llm(\n",
        "    llm=code_llm, prompt=prompt_RAG_tempate, retriever=retriever, return_source_documents=True\n",
        ")"
      ],
      "metadata": {
        "id": "azbvOUFRvEp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Try zero-shot prompt"
      ],
      "metadata": {
        "id": "3NBaObAQSlIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = code_llm.predict(text=user_question, max_output_tokens=2048, temperature=0.1)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1svTVwtBS0zP",
        "outputId": "234df47d-a693-4790-ac39-853e8882724d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```python\n",
            "def predict_with_langchain_llms(prompt):\n",
            "    \"\"\"Predicts the next token using LangChain LLMs interface with VertexAI text-bison model.\n",
            "\n",
            "    Args:\n",
            "        prompt: The prompt to predict the next token for.\n",
            "\n",
            "    Returns:\n",
            "        The predicted next token.\n",
            "    \"\"\"\n",
            "\n",
            "    # Create the LangChain LLMs interface.\n",
            "    llms = LangChainLLMs(\n",
            "        model_name=\"text-bison\",\n",
            "        project_id=\"YOUR_PROJECT_ID\",\n",
            "        location=\"YOUR_LOCATION\",\n",
            "    )\n",
            "\n",
            "    # Predict the next token.\n",
            "    prediction = llms.predict(prompt)\n",
            "\n",
            "    # Return the predicted next token.\n",
            "    return prediction\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Run prompt using RAG Chain & compare results.\n",
        "To generate response we use code-bison however can also use code-gecko and codechat-bison"
      ],
      "metadata": {
        "id": "JPm8qdxzwPM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = qa_chain({\"query\": user_question})\n",
        "print(results[\"result\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMz3nPMyVoj_",
        "outputId": "6ea6b550-365e-4147-c42a-7dcab4dde038"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```python\n",
            "def predict_with_langchain_llms_interface_with_vertexai_text_bison_model(\n",
            "    prompt: str,\n",
            ") -> str:\n",
            "    \"\"\"Create python function that takes a prompt and predicts using langchain.llms interface with VertexAI text-bison model\n",
            "\n",
            "    Args:\n",
            "        prompt (str): The prompt to predict\n",
            "\n",
            "    Returns:\n",
            "        str: The prediction\n",
            "    \"\"\"\n",
            "\n",
            "    # Initialize the VertexAI embeddings\n",
            "    embedding = VertexAIEmbeddings()\n",
            "\n",
            "    # Initialize the VertexAI LLM\n",
            "    llm = VertexAI(\n",
            "        model_name=\"text-bison-32k\",\n",
            "        max_output_tokens=256,\n",
            "        temperature=0.1,\n",
            "        top_p=0.8,\n",
            "        top_k=40,\n",
            "        verbose=True,\n",
            "    )\n",
            "\n",
            "    # Create the prediction\n",
            "    prediction = llm.predict(prompt)\n",
            "\n",
            "    return prediction\n",
            "\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's try another prompt"
      ],
      "metadata": {
        "id": "HF3lVWK1wjxe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_question = \"Create python function that takes text input and returns embeddings using Langchain with VertexAI textembedding-gecko model\"\n",
        "\n",
        "\n",
        "response = code_llm.predict(text=user_question, max_output_tokens=2048, temperature=0.1)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jel0ON68XiU7",
        "outputId": "cf6d8a16-4e03-4611-88ab-29be2bdde4e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```python\n",
            "import langchain\n",
            "from langchain.models import GeckoEmbeddings\n",
            "\n",
            "def get_embeddings(text):\n",
            "  # Load the Langchain model\n",
            "  model = GeckoEmbeddings()\n",
            "\n",
            "  # Generate embeddings for the input text\n",
            "  embeddings = model.encode(text)\n",
            "\n",
            "  return embeddings\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = qa_chain({\"query\": user_question})\n",
        "print(results[\"result\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9bIkqE8sO6P",
        "outputId": "a32b5812-387b-45ed-9491-ca381747903b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```python\n",
            "def get_embeddings_langchain_vertexai_textembedding_gecko(text):\n",
            "  \"\"\"Gets the embeddings for a given text using Langchain with VertexAI textembedding-gecko model.\n",
            "\n",
            "  Args:\n",
            "    text: The text to get the embeddings for.\n",
            "\n",
            "  Returns:\n",
            "    A list of embeddings.\n",
            "  \"\"\"\n",
            "\n",
            "  # Initialize the VertexAI embeddings model.\n",
            "  embedding = VertexAIEmbeddings()\n",
            "\n",
            "  # Get the embeddings for the text.\n",
            "  embeddings = embedding.embed_documents([text])\n",
            "\n",
            "  return embeddings[0]\n",
            "```\n"
          ]
        }
      ]
    }
  ]
}